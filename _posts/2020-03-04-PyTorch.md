---
layout: post
title:  "【python】PyTorch"
date:   2020-03-04 00:00:00 +0800
categories: toturial
tags: python pytorch
comments: 1
mathjax: true
---

<mark>PyTorch</mark>是一个Python的开源机器学习库。它用于自然语言处理等应用程序。它最初由Facebook人工智能研究小组开发，而优步的Pyro软件则用于概率编程。

最初，PyTorch由Hugh Perkins开发，作为基于Torch框架的LusJIT的Python包装器。有两种PyTorch变种。

PyTorch在Python中重新设计和实现Torch，同时为后端代码共享相同的核心C库。PyTorch开发人员调整了这个后端代码，以便有效地运行Python。他们还保留了基于GPU的硬件加速以及基于Lua的Torch的可扩展性功能。

## PyTorch功能特征

PyTorch的主要功能如下所述 - 

**简单的界面** -  PyTorch提供易于使用的API; 因此，它在Python上运行，操作非常简单。这个框架中的代码执行非常简单。

**Python用法** - PyTorch库认为是Pythonic，可以与Python数据科学堆栈平滑地集成。因此，它可以利用Python环境提供的所有服务和功能。

**计算图** -  PyTorch提供了一个提供动态计算图的出色平台。因此，用户可以在运行时更改它们。当开发人员不知道创建神经网络模型需要多少内存时，这时非常有用。

PyTorch以三个抽象级别而闻名，如下所示 -

- Tensor  - 在GPU上运行命令式n维数组。
- 变量 - 计算图中的节点，它存储数据和梯度。
- 模块 - 存储状态或可学习权重的神经网络层。

## PyTorch的优点

以下是PyTorch的优点 - 

- 它易于调试和理解代码。
- 它包括许多层作为Torch。
- 它包括许多损失函数。
- 它可以视为对GPU的NumPy扩展。
- 它允许构建其结构依赖于计算本身的网络。

## TensorFlow与PyTorch比较

下面是TensorFlow和PyTorch之间的主要区别 - 

| PyTorch                                                      | TensorFlow                                                   |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| PyTorch与基于lua的Torch框架密切相关，该框架在Facebook中广泛使用。 | TensorFlow由Google Brain开发，并在Google上积极使用。         |
| 与其他竞争技术相比，PyTorch相对较新。                        | TensorFlow并不是新的，但许多研究人员和行业专业人士视为一种前沿工具。 |
| PyTorch以强制性和动态的方式包含所有内容。                    | TensorFlow包含静态和动态图形作为组合。                       |
| PyTorch中的计算图是在运行时定义的。                          | TensorFlow不包含任何运行时选项。                             |
| PyTorch包括针对移动和嵌入式框架的部署。                      | TensorFlow更适用于嵌入式框架。                               |

# 数学构建模块

数学在任何机器学习算法中都是至关重要的，并且包括各种核心数学概念，以便以特定方式设计正确的算法。

下面提到了数学对机器学习和数据科学的重要性 - 

![数学相当](https://www.yiibai.com/uploads/article/2019/05/24/091509_24769.jpg)

现在，让我们来看看机器学习中的主要数学概念，从自然语言处理的角度来看这数学概念很重要 - 

## 矢量

向量(Vector)是连续或离散的数字数组，由向量组成的空间称为向量空间。向量的空间维度可以是有限的也可以是无限的，但机器学习和数据科学问题涉及固定长度向量。

矢量表示显示如下 - 

```python
temp = torch.FloatTensor([23,24,24.5,26,27.2,23.0])
temp.size()
#Output - torch.Size([6])
```

在机器学习中，经常要处理多维数据。因此，向量变得非常关键，并且被视为任何预测问题陈述的输入特征。

## 标量

标量(Scalar)是一个零维度，只包含一个值。在PyTorch中，它不包括零维度的特殊张量; 因此声明将如下 - 

```python
x = torch.rand(10)
x.size()
# Output - torch.Size([10])
```

## 矩阵

大多数结构化数据通常以表格或特定矩阵的形式表示。下面将使用一个名为Boston House Prices的数据集，该数据集可以在Python scikit-learn机器学习库中找到。

```python
boston_tensor = torch.from_numpy(boston.data)
boston_tensor.size()

#Output: torch.Size([506, 13])

boston_tensor[:2]

# Output:
# Columns 0 to 7
# 0.0063 18.0000 2.3100 0.0000 0.5380 6.5750 65.2000 4.0900
# 0.0273 0.0000 7.0700 0.0000 0.4690 6.4210 78.9000 4.9671
# Columns 8 to 12
# 1.0000 296.0000 15.3000 396.9000 4.9800
# 2.0000 242.0000 17.8000 396.9000 9.1400
```

# 神经网络基础

神经网络的主要原理包括一系列基本元素，即人工神经元或感知器。它包括几个基本输入，如：$x_1,x_2,\cdots,x_n$，如果总和大于激活潜在量，则产生二进制输出。

样本神经元的示意图如下所述 - 

![img](https://www.yiibai.com/uploads/article/2019/05/24/092410_95584.jpg)

产生的输出可以认为是具有激活潜在量或偏差加权和。

$$
Output=\sum_\limits{j}{w_jx_j}+Bias
$$

典型的神经网络架构如下所述 - 

![img](https://www.yiibai.com/uploads/article/2019/05/24/092606_64079.jpg)

输入和输出之间的层称为隐藏层，层之间的连接密度和类型是配置。例如，完全连接的配置使层$L$的所有神经元连接到$L+1$的神经元。对于更明显的定位，我们只能将一个局部邻域(比如九个神经元)连接到下一层。上图中显示了两个具有密集连接的隐藏层。

神经网络的类型如下 - 

#### 前馈神经网络

前馈神经网络包括神经网络族的基本单元。这种类型的神经网络中的数据移动是通过当前隐藏层从输入层到输出层。一层的输出用作输入层，对网络架构中的任何类型的环路都有限制。

![img](https://www.yiibai.com/uploads/article/2019/05/24/092738_85487.jpg)

#### 递归神经网络

递归神经网络是指数据模式在一段时间内发生变化的时间。在RNN中，应用相同的层来接受输入参数并在指定的神经网络中显示输出参数。

可以使用`torch.nn`包构建神经网络。

![img](https://www.yiibai.com/uploads/article/2019/05/24/092949_82030.jpg)

它是一个简单的前馈网络。它接受输入，一个接一个地通过几个层输入，然后最终给出输出。可以使用以下步骤进行神经网络的典型训练过程 - 

- 定义具有一些可学习参数(或权重)的神经网络。
- 迭代输入数据集。
- 通过网络处理输入。
- 计算损失(输出距离正确多远)。
- 将渐变传播回网络参数。
- 通常使用下面给出的简单更新来更新网络的权重。

```python
weight = weight -learning_rate * gradient
```

# 机器学习和深度学习

在本章中，我们将讨论机器和深度学习概念之间的主要区别。

## 数据量

机器学习使用不同数量的数据，主要用于少量数据。另一方面，如果数据量迅速增加，深度学习可以有效地工作。下图描绘了机器学习和深度学习在数据量方面的工作 - 

<img src="https://www.yiibai.com/uploads/article/2019/05/25/153030_71033.jpg" alt="机器学习与深度学习" style="zoom:67%;" />

## 硬件依赖

与传统的机器学习算法相反，深度学习算法设计为在很大程度上依赖于高端机器。深度学习算法执行大量矩阵乘法运算，这需要巨大的硬件支持。

## 特色工程

特征工程是将领域知识放入指定特征的过程，以降低数据的复杂性并制作学习算法可见的模式。

例如，传统的机器学习模式关注于特征工程过程所需的像素和其他属性。深度学习算法专注于数据的高级特征。它减少了为每个新问题开发新功能提取器的任务。

# 神经网络实现

PyTorch包含创建和实现神经网络的特殊功能。在本章中，我们将创建一个简单的神经网络，实现一个隐藏层开发单个输出单元。

我们将使用以下步骤使用PyTorch实现第一个神经网络 -

**第1步**

首先，需要使用以下命令导入PyTorch库 - 

```python
import torch 
import torch.nn as nn
```

**第2步**

定义所有图层和批量大小以开始执行神经网络，如下所示 - 

```python
# Defining input size, hidden layer size, output size and batch size respectively
n_in, n_h, n_out, batch_size = 10, 5, 1, 10
```

**第3步**

由于神经网络包含输入数据的组合以获得相应的输出数据，使用以下给出的相同程序 - 

```python
# Create dummy input and target tensors (data)
x = torch.randn(batch_size, n_in)
y = torch.tensor([[1.0], [0.0], [0.0], 
[1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])
```

**第4步**

借助内置函数创建顺序模型。使用下面的代码行，创建一个顺序模型 - 

```python
# Create a model
model = nn.Sequential(nn.Linear(n_in, n_h),
   nn.ReLU(),
   nn.Linear(n_h, n_out),
   nn.Sigmoid())
```

**第5步**

在Gradient Descent优化器的帮助下构建损失函数，如下所示 - 

```python
Construct the loss function
criterion = torch.nn.MSELoss()
# Construct the optimizer (Stochastic Gradient Descent in this case)
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)
```

**第6步**

使用给定代码行的迭代循环实现梯度下降模型 - 

```python
# Gradient Descent
for epoch in range(50):
   # Forward pass: Compute predicted y by passing x to the model
   y_pred = model(x)

   # Compute and print loss
   loss = criterion(y_pred, y)
   print('epoch: ', epoch,' loss: ', loss.item())

   # Zero gradients, perform a backward pass, and update the weights.
   optimizer.zero_grad()

   # perform a backward pass (backpropagation)
   loss.backward()

   # Update the parameters
   optimizer.step()
```

产生的输出如下 - 

```shell
epoch: 0 loss: 0.2545787990093231
epoch: 1 loss: 0.2545052170753479
epoch: 2 loss: 0.254431813955307
epoch: 3 loss: 0.25435858964920044
epoch: 4 loss: 0.2542854845523834
epoch: 5 loss: 0.25421255826950073
epoch: 6 loss: 0.25413978099823
epoch: 7 loss: 0.25406715273857117
epoch: 8 loss: 0.2539947032928467
epoch: 9 loss: 0.25392240285873413
epoch: 10 loss: 0.25385022163391113
epoch: 11 loss: 0.25377824902534485
epoch: 12 loss: 0.2537063956260681
epoch: 13 loss: 0.2536346912384033
epoch: 14 loss: 0.25356316566467285
epoch: 15 loss: 0.25349172949790955
epoch: 16 loss: 0.25342053174972534
epoch: 17 loss: 0.2533493936061859
epoch: 18 loss: 0.2532784342765808
epoch: 19 loss: 0.25320762395858765
epoch: 20 loss: 0.2531369626522064
epoch: 21 loss: 0.25306645035743713
epoch: 22 loss: 0.252996027469635
epoch: 23 loss: 0.2529257833957672
epoch: 24 loss: 0.25285571813583374
epoch: 25 loss: 0.25278574228286743
epoch: 26 loss: 0.25271597504615784
epoch: 27 loss: 0.25264623761177063
epoch: 28 loss: 0.25257670879364014
epoch: 29 loss: 0.2525072991847992
epoch: 30 loss: 0.2524380087852478
epoch: 31 loss: 0.2523689270019531
epoch: 32 loss: 0.25229987502098083
epoch: 33 loss: 0.25223103165626526
epoch: 34 loss: 0.25216227769851685
epoch: 35 loss: 0.252093642950058
epoch: 36 loss: 0.25202515721321106
epoch: 37 loss: 0.2519568204879761
epoch: 38 loss: 0.251888632774353
epoch: 39 loss: 0.25182053446769714
epoch: 40 loss: 0.2517525553703308
epoch: 41 loss: 0.2516847252845764
epoch: 42 loss: 0.2516169846057892
epoch: 43 loss: 0.2515493929386139
epoch: 44 loss: 0.25148195028305054
epoch: 45 loss: 0.25141456723213196
epoch: 46 loss: 0.2513473629951477
epoch: 47 loss: 0.2512802183628082
epoch: 48 loss: 0.2512132525444031
epoch: 49 loss: 0.2511464059352875
```

# 术语

在本章中，我们将了解PyTorch中一些最常用的术语。

#### PyTorch NumPy

PyTorch张量与NumPy阵列相同。张量(*tensor*)是一个n维数组，就PyTorch而言，它提供了许多在张量上运算的函数。

PyTorch张量通常利用GPU来加速其数值计算。在PyTorch中创建的这些张量可用于将双层网络适合随机数据。用户可以手动实现通过网络的前向和后向传递。

#### 变量和Autograd

使用自动编程时，网络的正向传递将定义计算图形 - 图形中的节点将是张量，边缘将是从输入张量产生输出张量的函数。

PyTorch张量可以创建为变量对象，其中变量表示计算图中的节点。

#### 动态图

静态图很好，因为用户可以预先优化图形。如果程序员一遍又一遍地重复使用相同的图形，则可以保持这种可能代价高昂的前期优化，因为相同的图形会一遍又一遍地重新运行。

它们之间的主要区别在于Tensor Flow的计算图是静态的，PyTorch使用动态计算图。

#### optim包

PyTorch中的optim包提取了一种优化算法的思想，该算法以多种方式实现，并提供了常用优化算法的说明。它可以在`import`语句中调用。

#### 多处理

多处理支持相同的操作，因此所有张量都可以在多个处理器上运行。队列将其数据移动到共享内存中，并仅向另一个进程发送句柄。

# 加载数据

PyTorch包含一个名为`torchvision`的包，用于加载和准备数据集。它包括两个基本功能，即`Dataset`和`DataLoader`，它们有助于数据集的转换和加载。

#### 数据集

数据集用于从给定数据集读取和转换数据点。实现的基本语法如下所述 - 

```python
trainset = torchvision.datasets.CIFAR10(root = './data', train = True,
   download = True, transform = transform)
```

`DataLoader`用于随机播放和批量处理数据。它可用于与多处理工作程序并行加载数据。

```python
trainloader = torch.utils.data.DataLoader(trainset, batch_size = 4,
   shuffle = True, num_workers = 2)
```

**示例：加载CSV文件**

使用Python包Panda来加载csv文件。原始文件具有以下格式:(图像名称，68个标记 - 每个标记具有`x`，`y`坐标)。

```python
landmarks_frame = pd.read_csv('faces/face_landmarks.csv')

n = 65
img_name = landmarks_frame.iloc[n, 0]
landmarks = landmarks_frame.iloc[n, 1:].as_matrix()
landmarks = landmarks.astype('float').reshape(-1, 2)
```

# 线性回归

在本章中，我们将重点介绍使用**TensorFlow**进行线性回归实现的基本示例。逻辑回归或线性回归是用于对离散类别进行分类的监督机器学习方法。在本章中的目标是构建一个模型，用户可以通过该模型预测预测变量与一个或多个自变量之间的关系。

如果$y$是因变量$x$而变化，则$X$认为是自变量。两个变量之间的这种关系可认为是线性的。两个变量的线性回归关系看起来就像下面提到的方程式一样 - 

$$
Y=Ax+b
$$

接下来，我们将设计一个线性回归算法，有助于理解下面给出的两个重要概念 - 

- 成本函数
- 梯度下降算法

下面提到线性回归的示意图，解释结果 - 

$$
y=ax+b
$$

- $a$的值是斜率。
- $b$的值是$y$ -截距。
- $r$是相关系数。
- $r^2$是相关系数。

线性回归方程的图形视图如下所述 - 

![线性回归方程](https://www.yiibai.com/uploads/article/2019/05/25/160235_58264.jpg)

以下步骤用于使用PyTorch实现线性回归 - 

**第1步**

使用以下代码导入在PyTorch中创建线性回归所需的包 - 

```python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import seaborn as sns
import pandas as pd
%matplotlib inline

sns.set_style(style = 'whitegrid')
plt.rcParams["patch.force_edgecolor"] = True
```

**第2步**

使用可用数据集创建单个训练集，如下所示 - 

```python
m = 2 # slope
c = 3 # interceptm = 2 # slope
c = 3 # intercept
x = np.random.rand(256)

noise = np.random.randn(256) / 4

y = x * m + c + noise

df = pd.DataFrame()
df['x'] = x
df['y'] = y

sns.lmplot(x ='x', y ='y', data = df)
```

得到的结果如下所示：

![img](https://www.yiibai.com/uploads/article/2019/05/25/160405_74769.jpg)

**第3步**

使用PyTorch库实现线性回归，如下所述 - 

```python
import torch
import torch.nn as nn
from torch.autograd import Variable
x_train = x.reshape(-1, 1).astype('float32')
y_train = y.reshape(-1, 1).astype('float32')

class LinearRegressionModel(nn.Module):
   def __init__(self, input_dim, output_dim):
      super(LinearRegressionModel, self).__init__()
      self.linear = nn.Linear(input_dim, output_dim)

   def forward(self, x):
      out = self.linear(x)
      return out
input_dim = x_train.shape[1]
output_dim = y_train.shape[1]
input_dim, output_dim(1, 1)
model = LinearRegressionModel(input_dim, output_dim)
criterion = nn.MSELoss()
[w, b] = model.parameters()

def get_param_values():
   return w.data[0][0], b.data[0]

def plot_current_fit(title = ""):
plt.figure(figsize = (12,4))
plt.title(title)
plt.scatter(x, y, s = 8)
w1 = w.data[0][0]
b1 = b.data[0]
x1 = np.array([0., 1.])
y1 = x1 * w1 + b1
plt.plot(x1, y1, 'r', label = 'Current Fit ({:.3f}, {:.3f})'.format(w1, b1))
plt.xlabel('x (input)')
plt.ylabel('y (target)')
plt.legend()
plt.show()
plot_current_fit('Before training')
```

执行上面示例代码，得到以下结果 - 

![img](https://www.yiibai.com/uploads/article/2019/05/25/160513_62351.jpg)

# CNN

深度学习是机器学习的一个分支，它是近几十年来研究人员突破的关键步骤。深度学习实现的示例包括图像识别和语音识别等应用。

下面给出了两种重要的深度神经网络 - 

- 卷积神经网络
- 递归神经网络。

在本章中，我们将关注第一种类型，即卷积神经网络(CNN)。

## 卷积神经网络

卷积神经网络旨在通过多层阵列处理数据。这种类型的神经网络用于图像识别或面部识别等应用。

CNN与任何其他普通神经网络之间的主要区别在于CNN将输入视为二维阵列并直接在图像上操作，而不是关注其他神经网络关注的特征提取。

CNN的主导方法包括识别问题的解决方案。谷歌和Facebook等顶级公司已投资于认可项目的研发项目，以更快的速度完成活动。

每个卷积神经网络都包含三个基本思想 - 

- 本地接收字段
- 卷积
- 池

下面来详细了解每个术语。

#### 1. 本地接收字段

CNN利用输入数据中存在的空间相关性。神经网络的并发层中的每一层都连接一些输入神经元。此特定区域称为**本地接收字段**。它只关注隐藏的神经元。隐藏的神经元将处理所提到的字段内的输入数据，而没有实现特定边界之外的变化。

生成本地相应字段的图表表示如下 - 

<img src="https://img-blog.csdn.net/20160117235851391" alt="img" style="zoom:80%;" />

<img src="https://img-blog.csdn.net/20160117235906854" style="zoom:80%;" />

#### 2. 卷积

在上图中，我们观察到每个连接都学习隐藏神经元的权重，并且具有从一个层到另一个层的移动的相关连接。在这里，各个神经元不时地进行转换。这个过程叫做“卷积”。

从输入层到隐藏特征映射的连接的映射被定义为“共享权重”，并且包括的偏差称为“共享偏差”。

#### 3. 池

卷积神经网络使用在CNN声明后立即定位的汇集层。它将来自用户的输入作为特征映射，其来自卷积网络并准备精简的特征映射。池化层有助于创建具有先前层的神经元的层。

#### 4. PyTorch实现

以下步骤用于使用PyTorch创建卷积神经网络。

**第1步**

导入必要的包以创建简单的神经网络。

```python
from torch.autograd import Variable
import torch.nn.functional as F
```

**第2步**
使用卷积神经网络的批处理表示创建一个类。输入`x`的批量形状的尺寸为`(3,32,32)`。

```python
class SimpleCNN(torch.nn.Module):
   def __init__(self):
      super(SimpleCNN, self).__init__()
      #Input channels = 3, output channels = 18
      self.conv1 = torch.nn.Conv2d(3, 18, kernel_size = 3, stride = 1, padding = 1)
      self.pool = torch.nn.MaxPool2d(kernel_size = 2, stride = 2, padding = 0)
      #4608 input features, 64 output features (see sizing flow below)
      self.fc1 = torch.nn.Linear(18 * 16 * 16, 64)
      #64 input features, 10 output features for our 10 defined classes
      self.fc2 = torch.nn.Linear(64, 10)
```

**第3步**

计算从(3,32,32)到(18,32,32)的第一卷积大小变化的激活。

尺寸的大小从(18,32,32)变为(18,16,16)。重塑神经网络输入层的数据维度，因为其大小从(18,16,16)变为(1,4608)。

回想一下`-1`从另一个给定的维度推断出这个维度。

```python
def forward(self, x):
   x = F.relu(self.conv1(x))
   x = self.pool(x)
   x = x.view(-1, 18 * 16 *16)
   x = F.relu(self.fc1(x))
   #Computes the second fully connected layer (activation applied later)
   #Size changes from (1, 64) to (1, 10)
   x = self.fc2(x)
   return(x)
```

# 递归神经网络

递归神经网络是一种遵循顺序方法的深度学习导向算法。在神经网络中，我们总是假设每个输入和输出都独立于所有其他层。这些类型的神经网络被称为循环，因为它们以顺序方式执行数学计算，完成一个接一个的任务。

下图说明了循环神经网络的完整方法和工作 -

<img src="https://www.yiibai.com/uploads/article/2019/05/25/161841_24179.jpg" alt="循环神经网络" style="zoom: 50%;" />

在上图中，`c1`，`c2`，`c3`和`x1`是包括一些隐藏输入值的输入，即输出`o1`的相应输出的`h1`，`h2`和`h3`。现在将专注于实现PyTorch，以在递归神经网络的帮助下创建正弦波。

在训练期间，将遵循模型的培训方法，一次只有一个数据点。输入序列`x`由`20`个数据点组成，并且目标序列与输入序列相同。

**第1步**

使用以下代码导入实现递归神经网络的必要包 - 

```python
import torch
from torch.autograd import Variable
import numpy as np
import pylab as pl
import torch.nn.init as init
```

**第2步**
设置模型超参数，输入层的大小设置为`7`。将有6个上下文神经元和1个输入神经元用于创建目标序列。

```python
dtype = torch.FloatTensor
input_size, hidden_size, output_size = 7, 6, 1
epochs = 300
seq_length = 20
lr = 0.1
data_time_steps = np.linspace(2, 10, seq_length + 1)
data = np.sin(data_time_steps)
data.resize((seq_length + 1, 1))

x = Variable(torch.Tensor(data[:-1]).type(dtype), requires_grad=False)
y = Variable(torch.Tensor(data[1:]).type(dtype), requires_grad=False)
```

我们将生成训练数据，其中`x`是输入数据序列，`y`是所需的目标序列。

**第3步**

使用具有零均值的正态分布在递归神经网络中初始化权重。`W1`表示接受输入变量，`w2`表示生成的输出，如下所示 - 

```python
w1 = torch.FloatTensor(input_size, 
hidden_size).type(dtype)
init.normal(w1, 0.0, 0.4)
w1 = Variable(w1, requires_grad = True)
w2 = torch.FloatTensor(hidden_size, output_size).type(dtype)
init.normal(w2, 0.0, 0.3)
w2 = Variable(w2, requires_grad = True)
```

**第4步**

现在，创建一个唯一定义神经网络的前馈功能非常重要。

```python
def forward(input, context_state, w1, w2):
   xh = torch.cat((input, context_state), 1)
   context_state = torch.tanh(xh.mm(w1))
   out = context_state.mm(w2)
   return (out, context_state)
```

**第5步**

下一步是开始循环神经网络正弦波实现的训练过程。外循环遍历每个循环，内循环遍历序列元素。在这里，还将计算均方误差(MSE)，它有助于预测连续变量。

```python
for i in range(epochs):
   total_loss = 0
   context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad = True)
   for j in range(x.size(0)):
      input = x[j:(j+1)]
      target = y[j:(j+1)]
      (pred, context_state) = forward(input, context_state, w1, w2)
      loss = (pred - target).pow(2).sum()/2
      total_loss += loss
      loss.backward()
      w1.data -= lr * w1.grad.data
      w2.data -= lr * w2.grad.data
      w1.grad.data.zero_()
      w2.grad.data.zero_()
      context_state = Variable(context_state.data)
   if i % 10 == 0:
      print("Epoch: {} loss {}".format(i, total_loss.data[0]))

context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad = False)
predictions = []

for i in range(x.size(0)):
   input = x[i:i+1]
   (pred, context_state) = forward(input, context_state, w1, w2)
   context_state = context_state
   predictions.append(pred.data.numpy().ravel()[0])
```

**第6步**
现在，以正确的方式绘制正弦波。

```python
pl.scatter(data_time_steps[:-1], x.data.numpy(), s = 90, label = "Actual")
pl.scatter(data_time_steps[1:], predictions, label = "Predicted")
pl.legend()
pl.show()
```

上述过程的输出如下 - 

<img src="https://www.yiibai.com/uploads/article/2019/05/25/162506_55856.jpg" alt="正弦波" style="zoom:80%;" />

# 数据集

在本章中，将更多地关注`torchvision.datasets`及其各种类型。PyTorch包括以下数据集加载器 - 

- MNIST
- COCO (字幕和检测)

数据集包括以下两种函数 - 

- `transform` - 一种接收图像并返回标准内容的修改版本的函数。这些可以与变换一起组合。
- `target_transform`  - 获取目标并对其进行转换的函数。例如，接受标题字符串并返回索引张量。

#### MNIST

以下是MNIST数据集的示例代码 - 

```python
dset.MNIST(root, train = TRUE, transform = NONE, 
target_transform = None, download = FALSE)
```

参数如下 - 

- `root` - 存在已处理数据的数据集的根目录。
- `train`  -  `True` =训练集，`False` =测试集
- `download`  -  `True` =从互联网下载数据集并将其放入根目录。

#### COCO

需要安装COCO API。以下示例用于演示使用PyTorch的数据集的COCO实现 - 

```python
import torchvision.dataset as dset
import torchvision.transforms as transforms
cap = dset.CocoCaptions(root = ' dir where images are', annFile = 'json annotation file', transform = transforms.ToTensor())

print('Number of samples: ', len(cap))
print(target)
```

上面程序代码输出结果如下：

```python
Number of samples: 82783
Image Size: (3L, 427L, 640L)
```

# Convents

Convents就是从 scratch 构建CNN模型。网络架构将包含以下步骤的组合 - 

- Conv2d
- MaxPool2d
- 整流线性单元
- 视图
- 线性层

## 训练模型

训练模型与图像分类问题相同。以下代码段完成了对提供的数据集的训练模型的过程 -

```python
def fit(epoch,model,data_loader,phase 
= 'training',volatile = False):
   if phase == 'training':
      model.train()
   if phase == 'training':
      model.train()
   if phase == 'validation':
      model.eval()
   volatile=True
   running_loss = 0.0
   running_correct = 0
   for batch_idx , (data,target) in enumerate(data_loader):
      if is_cuda:
         data,target = data.cuda(),target.cuda()
         data , target = Variable(data,volatile),Variable(target)
      if phase == 'training':
         optimizer.zero_grad()
         output = model(data)
         loss = F.nll_loss(output,target)
         running_loss + = 
         F.nll_loss(output,target,size_average = 
         False).data[0]
         preds = output.data.max(dim = 1,keepdim = True)[1]
         running_correct + = 
         preds.eq(target.data.view_as(preds)).cpu().sum()
         if phase == 'training':
            loss.backward()
            optimizer.step()
   loss = running_loss/len(data_loader.dataset)
   accuracy = 100. * running_correct/len(data_loader.dataset)
   print(f'{phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{return loss,accuracy}}')
```

此方法包括用于训练和验证的不同逻辑。使用不同模式有两个主要原因 - 

- 在训练模式中，丢失会删除一定百分比的值，这在验证或测试阶段不应发生。
- 对于训练模式，计算梯度并更改模型的参数值，但在测试或验证阶段不需要反向传播。

## 从Scratch训练

在本章中，我们将重点学习如何从Scratch创建一个Convent。这推断了使用 torch 创建相应的修道院或样本神经网络。

**第1步**

使用各自的参数创建必要的类，参数包括具有随机值的权重。

```python
class Neural_Network(nn.Module):
   def __init__(self, ):
      super(Neural_Network, self).__init__()
      self.inputSize = 2
      self.outputSize = 1
      self.hiddenSize = 3
      # weights
      self.W1 = torch.randn(self.inputSize, 
      self.hiddenSize) # 3 X 2 tensor
      self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor
```

**第2步**

使用`sigmoid`函数创建函数的向前模式。

```python
def forward(self, X):
   self.z = torch.matmul(X, self.W1) # 3 X 3 ".dot" 
   does not broadcast in PyTorch
   self.z2 = self.sigmoid(self.z) # activation function
   self.z3 = torch.matmul(self.z2, self.W2)
   o = self.sigmoid(self.z3) # final activation 
   function
   return o
   def sigmoid(self, s):
      return 1 / (1 + torch.exp(-s))
   def sigmoidPrime(self, s):
      # derivative of sigmoid
      return s * (1 - s)
   def backward(self, X, y, o):
      self.o_error = y - o # error in output
      self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error
      self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))
      self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)
      self.W1 + = torch.matmul(torch.t(X), self.z2_delta)
      self.W2 + = torch.matmul(torch.t(self.z2), self.o_delta)
```

**第3步**
创建如下所述的培训和预测模型 - 

```python
def train(self, X, y):
   # forward + backward pass for training
   o = self.forward(X)
   self.backward(X, y, o)
def saveWeights(self, model):
   # Implement PyTorch internal storage functions
   torch.save(model, "NN")
   # you can reload model with all the weights and so forth with:
   # torch.load("NN")
def predict(self):
   print ("Predicted data based on trained weights: ")
   print ("Input (scaled): " + str(xPredicted))
   print ("Output: " + str(self.forward(xPredicted)))
```

## 特征提取

卷积神经网络包括主要特征，提取。以下步骤用于实现卷积神经网络的特征提取。

**第1步**

导入相应的模型以使用“PyTorch”创建特征提取模型。

```python
import torch
import torch.nn as nn
from torchvision import models
```

**第2步**

创建一类特征提取器，可以在需要时调用。

```python
class Feature_extractor(nn.module):
   def forward(self, input):
      self.feature = input.clone()
      return input
new_net = nn.Sequential().cuda() # the new network
target_layers = [conv_1, conv_2, conv_4] # layers you want to extract`
i = 1
for layer in list(cnn):
   if isinstance(layer,nn.Conv2d):
      name = "conv_"+str(i)
      art_net.add_module(name,layer)
      if name in target_layers:
         new_net.add_module("extractor_"+str(i),Feature_extractor())
      i+=1
   if isinstance(layer,nn.ReLU):
      name = "relu_"+str(i)
      new_net.add_module(name,layer)
   if isinstance(layer,nn.MaxPool2d):
      name = "pool_"+str(i)
      new_net.add_module(name,layer)
new_net.forward(your_image)
print (new_net.extractor_3.feature)
```

## 可视化

在本章中，我们将在Convents的帮助下专注于数据可视化模型。需要以下步骤才能使用传统的神经网络获得完美的可视化图像。

**第1步**

导入必要的模块，这对于传统神经网络的可视化非常重要。

```python
import os
import numpy as np
import pandas as pd
from scipy.misc import imread
from sklearn.metrics import accuracy_score

import keras
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten, Activation, Input
from keras.layers import Conv2D, MaxPooling2D
import torch
```

**第2步**

要通过训练和测试数据来停止潜在的随机性，请调用以下代码中给出的相应数据集 - 

```python
seed = 128
rng = np.random.RandomState(seed)
data_dir = "../../datasets/MNIST"
train = pd.read_csv('../../datasets/MNIST/train.csv')
test = pd.read_csv('../../datasets/MNIST/Test_fCbTej3.csv')
img_name = rng.choice(train.filename)
filepath = os.path.join(data_dir, 'train', img_name)
img = imread(filepath, flatten=True)
```

#### 第3步

使用以下代码绘制必要的图像，以完美的方式定义训练和测试数据 - 

```python
pylab.imshow(img, cmap ='gray')
pylab.axis('off')
pylab.show()
```

输出显示如下 - 

<img src="https://www.yiibai.com/uploads/article/2019/05/25/171426_87254.jpg" alt="img" style="zoom:80%;" />

## 序列处理

在本章中，提出了一种替代方法，它依赖于跨两个序列的单个2D卷积神经网络。网络的每一层都根据到目前为止产生的输出序列重新编码源令牌。因此，类似注意的属性在整个网络中普遍存在。

在这里，将专注于使用数据集中包含的值创建具有特定池的顺序网络。此过程也最适用于“图像识别模块”。

![Convent进行序列处理](https://www.yiibai.com/uploads/article/2019/05/25/171614_73790.jpg)

以下步骤用于使用PyTorch创建带有Convent的序列处理模型 - 

**第1步**

使用convent导入必要的模块以执行序列处理。

```python
import keras 
from keras.datasets import mnist 
from keras.models import Sequential 
from keras.layers import Dense, Dropout, Flatten 
from keras.layers import Conv2D, MaxPooling2D 
import numpy as np
```

**第2步**

使用以下代码执行必要的操作以按相应的顺序创建模式 - 

```python
batch_size = 128 
num_classes = 10 
epochs = 12
# input image dimensions 
img_rows, img_cols = 28, 28
# the data, split between train and test sets 
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train.reshape(60000,28,28,1) 
x_test = x_test.reshape(10000,28,28,1)
print('x_train shape:', x_train.shape) 
print(x_train.shape[0], 'train samples') 
print(x_test.shape[0], 'test samples')
y_train = keras.utils.to_categorical(y_train, num_classes) 
y_test = keras.utils.to_categorical(y_test, num_classes)
```

**第3步**
编译模型并在所提到的传统神经网络模型中拟合模式，如下所示 - 

```python
model.compile(loss = 
keras.losses.categorical_crossentropy, 
optimizer = keras.optimizers.Adadelta(), metrics = 
['accuracy'])
model.fit(x_train, y_train, 
batch_size = batch_size, epochs = epochs, 
verbose = 1, validation_data = (x_test, y_test)) 
score = model.evaluate(x_test, y_test, verbose = 0) 
print('Test loss:', score[0]) 
print('Test accuracy:', score[1])
```

产生的输出如下 - 

<img src="https://www.yiibai.com/uploads/article/2019/05/25/171826_94262.jpg" alt="img" style="zoom:80%;" />

# 单词嵌入

在本章中，我们将了解单词嵌入模型—`word2vec`。Word2vec模型用于在相关模型组的帮助下生成单词嵌入。Word2vec模型使用纯C代码实现，并且手动计算梯度。

PyTorch中word2vec模型的实现在以下步骤中解释 - 

**第1步**

在以下库中实现单词嵌入，如下所述 - 

```python
import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
```

**第2步**

使用名为word2vec的类实现单词嵌入的Skip Gram模型。它包括：`emb_size`，`emb_dimension`，`u_embedding`，`v_embedding`类型的属性。

```python
class SkipGramModel(nn.Module):
   def __init__(self, emb_size, emb_dimension):
      super(SkipGramModel, self).__init__()
      self.emb_size = emb_size
      self.emb_dimension = emb_dimension
      self.u_embeddings = nn.Embedding(emb_size, emb_dimension, sparse=True)
      self.v_embeddings = nn.Embedding(emb_size, emb_dimension, sparse = True)
      self.init_emb()
   def init_emb(self):
      initrange = 0.5 / self.emb_dimension
      self.u_embeddings.weight.data.uniform_(-initrange, initrange)
      self.v_embeddings.weight.data.uniform_(-0, 0)
   def forward(self, pos_u, pos_v, neg_v):
      emb_u = self.u_embeddings(pos_u)
      emb_v = self.v_embeddings(pos_v)
      score = torch.mul(emb_u, emb_v).squeeze()
      score = torch.sum(score, dim = 1)
      score = F.logsigmoid(score)
      neg_emb_v = self.v_embeddings(neg_v)
      neg_score = torch.bmm(neg_emb_v, emb_u.unsqueeze(2)).squeeze()
      neg_score = F.logsigmoid(-1 * neg_score)
      return -1 * (torch.sum(score)+torch.sum(neg_score))
   def save_embedding(self, id2word, file_name, use_cuda):
      if use_cuda:
         embedding = self.u_embeddings.weight.cpu().data.numpy()
      else:
         embedding = self.u_embeddings.weight.data.numpy()
      fout = open(file_name, 'w')
      fout.write('%d %d' % (len(id2word), self.emb_dimension))
      for wid, w in id2word.items():
         e = embedding[wid]
         e = ' '.join(map(lambda x: str(x), e))
         fout.write('%s %s' % (w, e))
def test():
   model = SkipGramModel(100, 100)
   id2word = dict()
   for i in range(100):
      id2word[i] = str(i)
   model.save_embedding(id2word)
```

**第3步**

实现main方法，以正确的方式显示单词嵌入模型。

```python
if __name__  ==  '__main__':
   test()
```

# 递归神经网络

深度神经网络具有独特的功能，可以帮助机器学习突破自然语言的过程。 据观察，这些模型中的大多数将语言视为单词或字符的平坦序列，并使用一种称为递归神经网络或RNN的模型。

许多研究人员得出的结论是，对于短语的分层树，语言最容易被理解。 此类型包含在考虑特定结构的递归神经网络中。

PyTorch有一个特定的功能，有助于使这些复杂的自然语言处理模型更容易。 它是一个功能齐全的框架，适用于各种深度学习，并为计算机视觉提供强有力的支持。

**递归神经网络的特征**

以这样的方式创建递归神经网络，即它包括应用具有不同图形类似结构的相同组权重。

- 节点以拓扑顺序遍历。
- 这种类型的网络通过自动微分的反向模式进行训练。
- 自然语言处理包括递归神经网络的特殊情况。
- 递归神经张量网络包括树中的各种组合功能节点。

递归神经网络的例子如下所示 - 

<img src="https://www.yiibai.com/uploads/article/2019/05/25/172839_59505.jpg" alt="递归神经网络" style="zoom:80%;" />

