---
layout: post
title:  "【python】爬虫 <img src='https://img.shields.io/badge/-转载-C9284D?style=flat'> <img src='https://img.shields.io/badge/-推荐-F7DF1E?style=flat'>"
date:   2020-03-06 00:00:00 +0800
categories: toturial
tags: python 爬虫
comments: 1
mathjax: true
---

本文主要介绍python爬虫。

## requests库

因为有中文的官方文档，我就不介绍所有的功能了，只把常用到的说一下，大家用到更多功能的时候再去翻官方文档吧。

requests 库就是用来发送各种请求的，所以，我们就来看看各种请求怎么使用：

### get 请求

`r = requests.get("https://unsplash.com")`
这就是我们刚刚用到的。其实就是向网站发送了一个get请求，然后网站会返回一个response。r 就是response。大家可以在运行的时候查看r的type。
`print(type(r))`
![img](https://upload-images.jianshu.io/upload_images/3879861-b3ab5b3bdee9f0b7.png?imageMogr2/auto-orient/strip|imageView2/2/w/1240)
get请求还可以传递参数：

```python
payload = {'key1': 'value1', 'key2': 'value2'}
r = requests.get("http://httpbin.org/get", params=payload)
```

上面代码向服务器发送的请求中包含了两个参数key1和key2，以及两个参数的值。实际上它构造成了如下网址：
`http://httpbin.org/get?key1=value1&key2=value2`

### POST请求

无参数的post请求：
`r = requests.post("http://httpbin.org/post")`
有参数的post请求：

```python
payload = {'key1': 'value1', 'key2': 'value2'}
r = requests.post("http://httpbin.org/post", data=payload)
```

post请求多用来提交表单数据，即填写一堆输入框，然后提交。

### 其他请求

其他一些请求例如put请求、delete请求、head请求、option请求等其实都是类似的。但是平时用的不多，就不仔细介绍了。有用到的可以去看官网文档哦。阅读官方文档是必备技能！

```python
r = requests.put("http://httpbin.org/put")
r = requests.delete("http://httpbin.org/delete")
r = requests.head("http://httpbin.org/get")
r = requests.options("http://httpbin.org/get")
```

## BeautifulSoup 库

网上找到的几个官方文档：[BeautifulSoup4.4.0中文官方文档](http://beautifulsoup.readthedocs.io/zh_CN/latest/)，[BeautifulSoup4.2.0中文官方文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/)。不同版本的用法差不多，几个常用的语法都一样。

首先来看BeautifulSoup的对象种类，在使用的过程中就会了解你获取到的东西接下来应该如何操作。

### BeautifulSoup对象的类型

Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构，每个节点都是Python对象。所有对象可以归纳为4种类型: Tag , NavigableString , BeautifulSoup , Comment 。下面我们分别看看这四种类型都是什么东西。

#### Tag

这个就跟HTML或者XML（还能解析XML？是的，能！）中的标签是一样一样的。我们使用find()方法返回的类型就是这个（插一句：使用find-all()返回的是多个该对象的集合，是可以用for循环遍历的。）。返回标签之后，还可以对提取标签中的信息。

##### 提取标签的名字

```python
tag.name
```

##### 提取标签的属性

`tag['attribute']`
我们用一个例子来了解这个类型：

```python
from bs4 import BeautifulSoup

html_doc = """
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title"><b>The Dormouse's story</b></p>
<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>

<p class="story">...</p>
"""
soup = BeautifulSoup(html_doc, 'lxml')  #声明BeautifulSoup对象
find = soup.find('p')  #使用find方法查到第一个p标签
print("find's return type is ", type(find))  #输出返回值类型
print("find's content is", find)  #输出find获取的值
print("find's Tag Name is ", find.name)  #输出标签的名字
print("find's Attribute(class) is ", find['class'])  #输出标签的class属性值
```

#### NavigableString

NavigableString就是标签中的文本内容（不包含标签）。获取方式如下：
`tag.string`
还是以上面那个例子，加上下面这行，然后执行：
`print('NavigableString is：', find.string)`

![img](https://upload-images.jianshu.io/upload_images/3879861-f0f918aeedf2682d.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

#### BeautifulSoup

BeautifulSoup对象表示一个文档的全部内容。支持遍历文档树和搜索文档树。

#### Comment

这个对象其实就是HTML和XML中的注释。

```python
markup = "<b><!--Hey, buddy. Want to buy a used parser?--></b>"
soup = BeautifulSoup(markup)
comment = soup.b.string
type(comment)
# <class 'bs4.element.Comment'>
```

有些时候，我们并不想获取HTML中的注释内容，所以用这个类型来判断是否是注释。

```python
if type(SomeString) == bs4.element.Comment:
    print('该字符是注释')
else:
    print('该字符不是注释')
```

### BeautifulSoup遍历方法

#### 节点和标签名

可以使用子节点、父节点、 及标签名的方式遍历：

```python
soup.head #查找head标签
soup.p #查找第一个p标签

#对标签的直接子节点进行循环
for child in title_tag.children:
    print(child)

soup.parent #父节点

# 所有父节点
for parent in link.parents:
    if parent is None:
        print(parent)
    else:
        print(parent.name)

# 兄弟节点
sibling_soup.b.next_sibling #后面的兄弟节点
sibling_soup.c.previous_sibling #前面的兄弟节点

#所有兄弟节点
for sibling in soup.a.next_siblings:
    print(repr(sibling))

for sibling in soup.find(id="link3").previous_siblings:
    print(repr(sibling))
```

#### 搜索文档树

最常用的当然是find()和find_all()啦，当然还有其他的。比如find_parent() 和 find_parents()、 find_next_sibling() 和 find_next_siblings() 、find_all_next() 和 find_next()、find_all_previous() 和 find_previous() 等等。
我们就看几个常用的，其余的如果用到就去看官方文档哦。

- find_all()
  搜索当前tag的所有tag子节点，并判断是否符合过滤器的条件。返回值类型是bs4.element.ResultSet。
  完整的语法：
  `find_all( name , attrs , recursive , string , **kwargs )`
  这里有几个例子

```python
soup.find_all("title")
# [<title>The Dormouse's story</title>]
#
soup.find_all("p", "title")
# [<p class="title"><b>The Dormouse's story</b></p>]
# 
soup.find_all("a")
# [<a class="sister" href="http://example.com/elsie" id="link1">Elsie</a>,
#  <a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>,
#  <a class="sister" href="http://example.com/tillie" id="link3">Tillie</a>]
#
soup.find_all(id="link2")
# [<a class="sister" href="http://example.com/lacie" id="link2">Lacie</a>]
#
import re
soup.find(string=re.compile("sisters"))
# u'Once upon a time there were three little sisters; and their names were\n'
```

name 参数：可以查找所有名字为 name 的tag。
attr 参数：就是tag里的属性。
string 参数：搜索文档中字符串的内容。
recursive 参数： 调用tag的 find_all() 方法时，Beautiful Soup会检索当前tag的所有子孙节点。如果只想搜索tag的直接子节点，可以使用参数 recursive=False 。

- `find()`
  与`find_all()`类似，只不过只返回找到的第一个值。返回值类型是`bs4.element.Tag`。
  完整语法：
  `find( name , attrs , recursive , string , **kwargs )`
  看例子：

```python
soup.find('title')
# <title>The Dormouse's story</title>
#
soup.find("head").find("title")
# <title>The Dormouse's story</title>
```

### 实战

继续上一篇的网站Unsplash，我们在首页选中图片，查看html代码。发现所有的图片都在a标签里，并且class都是cV68d，如下图。

![img](https://upload-images.jianshu.io/upload_images/3879861-9c048de3d30d4725.png?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

通过仔细观察，发现图片的链接在style中的background-image中有个url。这个url就包含了图片的地址，url后面跟了一堆参数，可以看到其中有&w=XXX&h=XXX，这个是宽度和高度参数。我们把高度和宽度的参数去掉，就能获取到大图。下面，我们先获取到所有的含有图片的a标签，然后在循环获取a标签中的style内容。

其实在图片的右下方有一个下载按钮，按钮的标签中有一个下载链接，但是该链接并不能直接请求到图片，需要跳转几次，通过获取表头里的Location才能获取到真正的图片地址。后续我再以这个角度获取图片写一篇博文，咱们现根据能直接获取到的url稍做处理来获取图片。小伙伴儿们也可能会发现其他的方式来获取图片的url，都是可以的，尽情的尝试吧！

```python
import requests #导入requests 模块
from bs4 import BeautifulSoup  #导入BeautifulSoup 模块

headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36'}  #给请求指定一个请求头来模拟chrome浏览器
web_url = 'https://unsplash.com'r = requests.get(web_url, headers=headers) #像目标url地址发送get请求，返回一个response对象
all_a = BeautifulSoup(r.text, 'lxml').find_all('a', class_='cV68d')  #获取网页中的class为cV68d的所有a标签
for a in all_a:
  print(a['style']) #循环获取a标签中的style
```

这里的find_all('a', class_='cV68d') 是找到所有class为cV68d的a标签，返回的是一个list，所以可以用for循环获取每个a标签。
还有，get请求使用了headers参数，这个是用来模拟浏览器的。如何知道‘User-Agent’是什么呢？
在你的Chrome浏览器中，按F12，然后刷新网页，看下图就可以找到啦。

![img](https://upload-images.jianshu.io/upload_images/3879861-7d4c19a99770ea20.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

OK，我们来执行以下上面的代码，结果如下：
![img](https://upload-images.jianshu.io/upload_images/3879861-d1d6d51e980f29a7.png?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

接下来的任务是在一行的文本中取到图片的url。仔细看每一行的字符串，两个双引号之间的内容就是图片的url了，所以我们Python的切片功能来截取这中间的内容。

改写for循环中的内容：

```python
for a in all_a: 
    img_str = a['style'] #a标签中完整的style字符串
    print(img_str[img_str.index('"')+1 : img_str.index('"',img_str[img_str.index('"')+1)]) #使用Python的切片功能截取双引号之间的内容
```

获取到url后还要把宽度和高度的参数去掉。

```python
        for a in all_a:
            img_str = a['style'] #a标签中完整的style字符串
            print('a标签的style内容是：', img_str)
            first_pos = img_str.index('"') + 1
            second_pos = img_str.index('"',first_pos)
            img_url = img_str[first_pos: second_pos] #使用Python的切片功能截取双引号之间的内容
            width_pos = img_url.index('&w=')
            height_pos = img_url.index('&q=')
            width_height_str = img_url[width_pos : height_pos]
            print('高度和宽度数据字符串是：', width_height_str)
            img_url_final = img_url.replace(width_height_str, '')
            print('截取后的图片的url是：', img_url_final)
```

有了这些图片的url，就可以通过继续发请求的方式获取图片啦。接下来我们先来封装一下发请求的代码。
先创建一个类：

```python
class BeautifulPicture(): 
   def __init__(self):  #类的初始化操作
        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}  #给请求指定一个请求头来模拟chrome浏览器
        self.web_url = 'https://unsplash.com' #要访问的网页地址
        self.folder_path = 'D:\BeautifulPicture'  #设置图片要存放的文件目录
```

然后封装request请求：

```python
    def request(self, url):  #返回网页的response
        r = requests.get(url)  # 像目标url地址发送get请求，返回一个response对象
        return r
```

我们在文件目录下保存图片的话，要先创建文件目录。所以再添加一个创建目录的方法：
要先引入os库哦。
`import os`
然后是方法定义：

```python
    def mkdir(self, path):  ##这个函数创建文件夹
        path = path.strip()
        isExists = os.path.exists(path)
        if not isExists:
            print('创建名字叫做', path, '的文件夹')
            os.makedirs(path)
            print('创建成功！')
        else:
            print(path, '文件夹已经存在了，不再创建')
```

再然后是保存图片啦。

```python
    def save_img(self, url, name): ##保存图片
        print('开始保存图片...')
        img = self.request(url)
        time.sleep(5)
        file_name = name + '.jpg'
        print('开始保存文件')
        f = open(file_name, 'ab')
        f.write(img.content)
        print(file_name,'文件保存成功！')
        f.close()
```

工具方法都已经准备完毕，开始我们的逻辑部分：

```python
    def get_pic(self):
        print('开始网页get请求')
        r = self.request(self.web_url)
        print('开始获取所有a标签')
        all_a = BeautifulSoup(r.text, 'lxml').find_all('a', class_='cV68d')  #获取网页中的class为cV68d的所有a标签
        print('开始创建文件夹')
        self.mkdir(self.folder_path)  #创建文件夹
        print('开始切换文件夹')
        os.chdir(self.folder_path)   #切换路径至上面创建的文件夹
        i = 1 #后面用来给图片命名
        for a in all_a:
            img_str = a['style'] #a标签中完整的style字符串
            print('a标签的style内容是：', img_str)
            first_pos = img_str.index('"') + 1
            second_pos = img_str.index('"',first_pos)
            img_url = img_str[first_pos: second_pos] #使用Python的切片功能截取双引号之间的内容
            width_pos = img_url.index('&w=')
            height_pos = img_url.index('&q=')
            width_height_str = img_url[width_pos : height_pos]
            print('高度和宽度数据字符串是：', width_height_str)
            img_url_final = img_url.replace(width_height_str, '')
            print('截取后的图片的url是：', img_url_final)
            self.save_img(img_url_final, str(i))
            i += 1
```

最后就是执行啦：

```python
beauty = BeautifulPicture()  #创建一个类的实例
beauty.get_pic()  #执行类中的方法
```

最后来一个完整的代码，对中间的一些部分进行了封装和改动，并添加了每部分的注释，一看就明白了。有哪块有疑惑的可以留言~~

```python
import requests #导入requests 模块
from bs4 import BeautifulSoup  #导入BeautifulSoup 模块
import os  #导入os模块

class BeautifulPicture():

    def __init__(self):  #类的初始化操作
        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1'}  #给请求指定一个请求头来模拟chrome浏览器
        self.web_url = 'https://unsplash.com'  #要访问的网页地址
        self.folder_path = 'D:\BeautifulPicture'  #设置图片要存放的文件目录

    def get_pic(self):
        print('开始网页get请求')
        r = self.request(self.web_url)
        print('开始获取所有a标签')
        all_a = BeautifulSoup(r.text, 'lxml').find_all('a', class_='cV68d')  #获取网页中的class为cV68d的所有a标签
        print('开始创建文件夹')
        self.mkdir(self.folder_path)  #创建文件夹
        print('开始切换文件夹')
        os.chdir(self.folder_path)   #切换路径至上面创建的文件夹

        for a in all_a: #循环每个标签，获取标签中图片的url并且进行网络请求，最后保存图片
            img_str = a['style'] #a标签中完整的style字符串
            print('a标签的style内容是：', img_str)
            first_pos = img_str.index('"') + 1
            second_pos = img_str.index('"',first_pos)
            img_url = img_str[first_pos: second_pos] #使用Python的切片功能截取双引号之间的内容
            #获取高度和宽度的字符在字符串中的位置
            width_pos = img_url.index('&w=')
            height_pos = img_url.index('&q=')
            width_height_str = img_url[width_pos : height_pos] #使用切片功能截取高度和宽度参数，后面用来将该参数替换掉
            print('高度和宽度数据字符串是：', width_height_str)
            img_url_final = img_url.replace(width_height_str, '')  #把高度和宽度的字符串替换成空字符
            print('截取后的图片的url是：', img_url_final)
            #截取url中参数前面、网址后面的字符串为图片名
            name_start_pos = img_url.index('photo')
            name_end_pos = img_url.index('?')
            img_name = img_url[name_start_pos : name_end_pos]
            self.save_img(img_url_final, img_name) #调用save_img方法来保存图片

    def save_img(self, url, name): ##保存图片
        print('开始请求图片地址，过程会有点长...')
        img = self.request(url)
        file_name = name + '.jpg'
        print('开始保存图片')
        f = open(file_name, 'ab')
        f.write(img.content)
        print(file_name,'图片保存成功！')
        f.close()

    def request(self, url):  #返回网页的response
        r = requests.get(url, headers=self.headers)  # 像目标url地址发送get请求，返回一个response对象。有没有headers参数都可以。
        return r

    def mkdir(self, path):  ##这个函数创建文件夹
        path = path.strip()
        isExists = os.path.exists(path)
        if not isExists:
            print('创建名字叫做', path, '的文件夹')
            os.makedirs(path)
            print('创建成功！')
        else:
            print(path, '文件夹已经存在了，不再创建')

beauty = BeautifulPicture()  #创建类的实例
beauty.get_pic()  #执行类中的方法
```

执行的过程中可能会有点慢，这是因为图片本身比较大！如果仅仅是为了测试爬虫，则可以不把图片的宽度和高度替换掉，图片就没那么大啦，运行过程会快很多。

## PhatomJS

在网上搜了一下，没有找到中文的官网文档，只找到了[PhatomJS英文官方文档](http://phantomjs.org/quick-start.html)。

先来一个官网上的例子，PhatomJS执行js文件，输出“Hello， World！”
新建一个js文件，包含下面的代码，然后保存为hello.js。

```python
console.log('Hello, world!');
phantom.exit();  #用来终止phtomjs程序
```

打开cmd命令窗口，进入到hello.js 文件所在目录，执行下面的命令：
`phantomjs hello.js`
输出：
`Hello, world!`

这个例子虽然方法比较笨，需要先创建一个js文件，然后再去执行。但是演示了PhatomJS的一个很重要的功能，就是执行js代码。我们后面会用到它来执行下拉操作哦。

我们使用PhatomJS来请求一个网页，然后把网页截图保存。
创建一个js文件，包含下面的代码，保存为music.js

```python
var page = require('webpage').create();
page.open('http://music.163.com/', function(status) {
  console.log("Status: " + status);
  if(status === "success") {
    page.render('music.png');
  }
  phantom.exit();
});
```

然后使用cmd命令窗口，在music.js文件所在目录，执行下面的命令：
`phatomjs music.js`
这样会有一个屏幕截图保存在文件所在的文件夹喽。

## Selenium

Selenium支持很多语言的使用，上面也提到了。其他语言的使用我没有接触过，直接看Python语言的使用。官网：[Selenium with Python](http://selenium-python.readthedocs.io/index.html)。

### 使用方法

因为PhatomJS没有界面，在使用的时候没有直观感受。所以，我们暂时使用Chrome浏览器代替PhatomJS。

首先下载Chrome浏览器驱动：[下载地址](http://chromedriver.storage.googleapis.com/index.html?path=2.9/)，选择自己的系统版本，然后下载。这是一个压缩包，解压后存到一个目录中，然后把该目录**添加到环境变量**。因为上面的PhatomJS已经演示过添加环境变量的流程，这里就不赘述了。

在PyCharm中输入下面代码，然后运行看看有什么反应。

```python
from selenium import webdriver  #导入Selenium的webdriver
from selenium.webdriver.common.keys import Keys  #导入Keys

driver = webdriver.Chrome()  #指定使用的浏览器，初始化webdriver
driver.get("http://www.python.org")  #请求网页地址
assert "Python" in driver.title  #看看Python关键字是否在网页title中，如果在则继续，如果不在，程序跳出。
elem = driver.find_element_by_name("q")  #找到name为q的元素，这里是个搜索框
elem.clear()  #清空搜索框中的内容
elem.send_keys("pycon")  #在搜索框中输入pycon
elem.send_keys(Keys.RETURN)  #相当于回车键，提交
assert "No results found." not in driver.page_source  #如果当前页面文本中有“No results found.”则程序跳出
driver.close()  #关闭webdriver
```

哎呀，其实浏览器的变化流程我都写到上面的代码的注释中了，就不再说一遍了哦。

通过上面的例子，我们可以大概的了解了Selenium都能做什么。个人认为其中比较重要的就是定位，即找到页面中的元素，然后进行相关操作。

### 导航操作 Navigating

导航操作的使用非常简单，完全可以通名字就知道它们是干什么的，不信先给你几个例子瞧瞧：

```python
element = driver.find_element_by_id("passwd-id")  #通过id获取元素

element = driver.find_element_by_name("passwd")  #通过name获取元素

element = driver.find_element_by_xpath("//input[@id='passwd-id']")  #通过使用xpath匹配获取元素
```

下面列出详细的定位方法。有定位一个元素的，也有定位多个元素的。

- 定位一个元素：

```python
find_element_by_id
find_element_by_name
find_element_by_xpath
find_element_by_link_text
find_element_by_partial_link_text
find_element_by_tag_name
find_element_by_class_name
find_element_by_css_selector
```

- 定位多个元素：

```python
find_elements_by_name
find_elements_by_xpath
find_elements_by_link_text
find_elements_by_partial_link_text
find_elements_by_tag_name
find_elements_by_class_name
find_elements_by_css_selector
```

通过导航操作，我们就可以在找到想找的元素，然后进行接下来的处理，比如，想在一个输入框中输入数据。看3.1的那个栗子，通过导航，找到了name为p的输入框，然后使用`send_keys`在输入框中写入数据：
`elem.send_keys("pycon")`

我们知道了如何在输入框中输入数据，那如果我们碰到了下拉框该怎么办呢？

```python
from selenium.webdriver.support.ui import Select   #导入Select
select = Select(driver.find_element_by_name('name'))  #通过Select来定义该元素是下拉框
select.select_by_index(index)  #通过下拉元素的位置来选择
select.select_by_visible_text("text")  #通过下拉元素的内容来选择
select.select_by_value(value)  #通过下拉元素的取值来选择
```

来看一个下拉框的HTML：

```html
<select name="cars">
  <option value ="volvo">沃尔沃</option>
  <option value ="bmw">宝马</option>
  <option value="benz">奔驰</option>
  <option value="audi">奥迪</option>
</select>
```

结合上面的例子：

```python
from selenium.webdriver.support.ui import Select  
select = Select(driver.find_element_by_name('cars'))  #找到name为cars的select标签
select.select_by_index(1)  #下拉框选中沃尔沃
select.select_by_visible_text("宝马")  #下拉框选中宝马
select.select_by_value("benz")  #下拉框选中奥迪
```

嗯...，让我想想还有什么... ...。对了，还有一个操作特别常用，就是点击操作。这个很简单：
`elem.click()`

在找到的元素后面加上click()就可以了。

### Cookies

我们想要爬取的网站有些可能需要登录，这样就需要在请求网站的时候添加Cookies。

```python
driver.get("http://www.example.com") #先请求一个网页

cookie = {‘name’ : ‘foo’, ‘value’ : ‘bar’} #设置cookie内容
driver.add_cookie(cookie)  #添加cookie
```

## 静态js实战

------

### 模拟下拉操作

要想实现网页的下拉操作，需要使用Selenium的一个方法来执行js代码。该方法如下：
`driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")`

由此可见，使用`execute_script`方法可以调用JavaScript API在一个加载完成的页面中去执行js代码。可以做任何你想做的操作哦，只要用js写出来就可以了。

改造的爬虫的第一步就是封装一个下拉方法，这个方法要能控制下拉的次数，下拉后要有等待页面加载的时间，以及做一些信息记录（在看下面的代码前自己先想一想啦）：

```python
    def scroll_down(self, driver, times):
        for i in range(times):
            print("开始执行第", str(i + 1),"次下拉操作")
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")  #执行JavaScript实现网页下拉倒底部
            print("第", str(i + 1), "次下拉操作执行完毕")
            print("第", str(i + 1), "次等待网页加载......")
            time.sleep(20)  # 等待20秒（时间可以根据自己的网速而定），页面加载出来再执行下拉操作
```

这部分做完之后就是修改页面爬取逻辑，之前是使用request 请求网址，然后找到图片url所在位置，再然后挨个请求图片url。现在，我们要首先使用Selenium 请求网址，然后模拟下拉操作，等待页面加载完毕再遵循原有逻辑挨个请求图片的url。

逻辑部分改造如下：

```python
    def get_pic(self):
        print('开始网页get请求')
        # 使用selenium通过PhantomJS来进行网络请求
        driver = webdriver.PhantomJS()
        driver.get(self.web_url)
        self.scroll_down(driver=driver, times=5)  #执行网页下拉到底部操作，执行5次
        print('开始获取所有a标签')
        all_a = BeautifulSoup(driver.page_source, 'lxml').find_all('a', class_='cV68d')  #获取网页中的class为cV68d的所有a标签
        print('开始创建文件夹')
        self.mkdir(self.folder_path)  #创建文件夹
        print('开始切换文件夹')
        os.chdir(self.folder_path)   #切换路径至上面创建的文件夹

        print("a标签的数量是：", len(all_a))  #这里添加一个查询图片标签的数量，来检查我们下拉操作是否有误
        for a in all_a: #循环每个标签，获取标签中图片的url并且进行网络请求，最后保存图片
            img_str = a['style'] #a标签中完整的style字符串
            print('a标签的style内容是：', img_str)
            first_pos = img_str.index('"') + 1  #获取第一个双引号的位置，然后加1就是url的起始位置
            second_pos = img_str.index('"', first_pos)  #获取第二个双引号的位置
            img_url = img_str[first_pos: second_pos] #使用Python的切片功能截取双引号之间的内容

            #注：为了尽快看到下拉加载的效果，截取高度和宽度部分暂时注释掉，因为图片较大，请求时间较长。
            ##获取高度和宽度的字符在字符串中的位置
            #width_pos = img_url.index('&w=')
            #height_pos = img_url.index('&q=')
            #width_height_str = img_url[width_pos : height_pos] #使用切片功能截取高度和宽度参数，后面用来将该参数替换掉
            #print('高度和宽度数据字符串是：', width_height_str)
            #img_url_final = img_url.replace(width_height_str, '')  #把高度和宽度的字符串替换成空字符
            #print('截取后的图片的url是：', img_url_final)

            #截取url中参数前面、网址后面的字符串为图片名
            name_start_pos = img_url.index('photo')
            name_end_pos = img_url.index('?')
            img_name = img_url[name_start_pos : name_end_pos]
            self.save_img(img_url, img_name) #调用save_img方法来保存图片
```

逻辑修改完毕。执行一下，发现报错了，图片的url截取错误。那就看看到底是为什么，先输出找到url看看：

![img](https://upload-images.jianshu.io/upload_images/3879861-e70c7fa3cd56af0e.png?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

看看输出内容，发现通过PhatomJS 请求网页获得的url([https://blablabla](https://blablabla/)) 中，竟然没有双引号，这跟使用Chrome 看到的不一样。好吧，那就按照没有双引号的字符串重新截取图片的url。

其实，我们只需要把url的起始位置和结束逻辑改成通过小括号来获取就可以了：

```
first_pos = img_str.index('(') + 1  #起始位置是小括号的左边
second_pos = img_str.index(')')  #结束位置是小括号的右边
```

好啦，这次获取图片的url就没什么问题了，程序可以顺利的执行。

但是，细心的小伙伴儿肯定发现了，我们这个爬虫在爬取的过程中中断了，或者过一段时间网站图片更新了，我再想爬，有好多图片都是重复的，却又重新爬取一遍，浪费时间。那么我们有什么办法来达到去重的效果呢？

### 去重

想要做去重，无非就是在爬取图片的时候记录图片名字（图片名字是唯一的）。在爬取图片前，先检查该图片是否已经爬取过，如果没有，则继续爬取，如果已经爬取过，则不进行爬取。

去重的逻辑如上，实现方式的不同主要体现在记录已经爬取过的信息的途径不同。比如可以使用数据库记录、使用log文件记录，或者直接检查已经爬取过的数据信息。

根据爬取内容的不同实现方式也不同。我们这里先使用去文件夹下获取所有文件名，然后在爬取的时候做对比。
后面的爬虫实战再使用其他方式来做去重。

单从爬取unsplash 网站图片这个实例来看，需要知道文件夹中所有文件的名字就够了。

Python os 模块中有两种能够获取文件夹下所有文件名的方法，分别来个示例：

```python
for root, dirs, files in os.walk(path):
    for file in files:
        print(file)
for file in os.listdir(path):
    print(file)
```

其中，os.walk(path) 是获取path文件夹下所有文件名和所有其子目录中的文件夹名和文件名。
而os.listdir(path) 只是获取path文件夹下的所有文件的名字，并不care其子文件夹。

这里我们使用os.listdir(path) 就能满足需求。

写一个获取文件夹内所有文件名的方法：

```python
    def get_files(self, path):
        pic_names = os.listdir(path)
        return pic_names
```

因为在保存图片之前就要用到文件名的对比，所以把save_img方法修改了一下，在方法外部拼接图片名字，然后直接作为参数传入，而不是在图片存储的过程中命名：

```python
    def save_img(self, url, file_name): ##保存图片
        print('开始请求图片地址，过程会有点长...')
        img = self.request(url)
        print('开始保存图片')
        f = open(file_name, 'ab')
        f.write(img.content)
        print(file_name,'图片保存成功！')
        f.close()
```

为了更清晰去重逻辑，我们每次启动爬虫的时候检测一下图片存放的文件夹是否存在，如果存在则检测与文件夹中的文件做对比，如果不存在则不需要做对比（因为是新建的嘛，文件夹里面肯定什么文件都没有）。
逻辑修改如下，是新建的则返回True，不是新建的则返回False：

```python
    def mkdir(self, path):  ##这个函数创建文件夹
        path = path.strip()
        isExists = os.path.exists(path)
        if not isExists:
            print('创建名字叫做', path, '的文件夹')
            os.makedirs(path)
            print('创建成功！')
            return True
        else:
            print(path, '文件夹已经存在了，不再创建')
            return False
```

然后修改我们的爬取逻辑部分：
主要是添加获取的文件夹中的文件名列表，然后判断图片是否存在。

```python
is_new_folder = self.mkdir(self.folder_path)  #创建文件夹，并判断是否是新创建
file_names = self.get_files(self.folder_path)  #获取文件家中的所有文件名，类型是list
            if is_new_folder:
                self.save_img(img_url, img_name)  # 调用save_img方法来保存图片
            else:
                if img_name not in file_names:
                    self.save_img(img_url, img_name)  # 调用save_img方法来保存图片
                else:
                    print("该图片已经存在：", img_name, "，不再重新下载。")
```

好了，来个完整版代码（也可以去 [GitHub下载](https://github.com/AlbertShoubinLi/UnsplashSpider)）：

```python
from selenium import webdriver  #导入Selenium
import requests
from bs4 import BeautifulSoup  #导入BeautifulSoup 模块
import os  #导入os模块
import time

class BeautifulPicture():

    def __init__(self):  #类的初始化操作
        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1'}  #给请求指定一个请求头来模拟chrome浏览器
        self.web_url = 'https://unsplash.com'  #要访问的网页地址
        self.folder_path = 'C:\D\BeautifulPicture'  #设置图片要存放的文件目录

    def get_pic(self):
        print('开始网页get请求')
        # 使用selenium通过PhantomJS来进行网络请求
        driver = webdriver.PhantomJS()
        driver.get(self.web_url)
        self.scroll_down(driver=driver, times=3)  #执行网页下拉到底部操作，执行3次
        print('开始获取所有a标签')
        all_a = BeautifulSoup(driver.page_source, 'lxml').find_all('a', class_='cV68d')  #获取网页中的class为cV68d的所有a标签
        print('开始创建文件夹')
        is_new_folder = self.mkdir(self.folder_path)  #创建文件夹，并判断是否是新创建
        print('开始切换文件夹')
        os.chdir(self.folder_path)   #切换路径至上面创建的文件夹

        print("a标签的数量是：", len(all_a))   #这里添加一个查询图片标签的数量，来检查我们下拉操作是否有误
        file_names = self.get_files(self.folder_path)  #获取文件家中的所有文件名，类型是list

        for a in all_a: #循环每个标签，获取标签中图片的url并且进行网络请求，最后保存图片
            img_str = a['style'] #a标签中完整的style字符串
            print('a标签的style内容是：', img_str)
            first_pos = img_str.index('(') + 1
            second_pos = img_str.index(')')
            img_url = img_str[first_pos: second_pos] #使用Python的切片功能截取双引号之间的内容

            # 注：为了尽快看到下拉加载的效果，截取高度和宽度部分暂时注释掉，因为图片较大，请求时间较长。
            #获取高度和宽度的字符在字符串中的位置
            # width_pos = img_url.index('&w=')
            # height_pos = img_url.index('&q=')
            # width_height_str = img_url[width_pos : height_pos] #使用切片功能截取高度和宽度参数，后面用来将该参数替换掉
            # print('高度和宽度数据字符串是：', width_height_str)
            # img_url_final = img_url.replace(width_height_str, '')  #把高度和宽度的字符串替换成空字符
            # print('截取后的图片的url是：', img_url_final)

            #截取url中参数前面、网址后面的字符串为图片名
            name_start_pos = img_url.index('.com/') + 5  #通过找.com/的位置，来确定它之后的字符位置
            name_end_pos = img_url.index('?')
            img_name = img_url[name_start_pos : name_end_pos] + '.jpg'
            img_name = img_name.replace('/','')  #把图片名字中的斜杠都去掉

            if is_new_folder:
                self.save_img(img_url, img_name)  # 调用save_img方法来保存图片
            else:
                if img_name not in file_names:
                    self.save_img(img_url, img_name)  # 调用save_img方法来保存图片
                else:
                    print("该图片已经存在：", img_name, "，不再重新下载。")

    def save_img(self, url, file_name): ##保存图片
        print('开始请求图片地址，过程会有点长...')
        img = self.request(url)
        print('开始保存图片')
        f = open(file_name, 'ab')
        f.write(img.content)
        print(file_name,'图片保存成功！')
        f.close()

    def request(self, url):  #返回网页的response
        r = requests.get(url)  # 像目标url地址发送get请求，返回一个response对象。有没有headers参数都可以。
        return r

    def mkdir(self, path):  ##这个函数创建文件夹
        path = path.strip()
        isExists = os.path.exists(path)
        if not isExists:
            print('创建名字叫做', path, '的文件夹')
            os.makedirs(path)
            print('创建成功！')
            return True
        else:
            print(path, '文件夹已经存在了，不再创建')
            return False

    def scroll_down(self, driver, times):
        for i in range(times):
            print("开始执行第", str(i + 1),"次下拉操作")
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")  #执行JavaScript实现网页下拉倒底部
            print("第", str(i + 1), "次下拉操作执行完毕")
            print("第", str(i + 1), "次等待网页加载......")
            time.sleep(30)  # 等待30秒，页面加载出来再执行下拉操作

    def get_files(self, path):
        pic_names = os.listdir(path)
        return pic_names

beauty = BeautifulPicture()  #创建类的实例
beauty.get_pic()  #执行类中的方法
```

## 动态js实战

上面提到过，网易云音乐的网页跟普通的网页相比主要有两点不同：

- 网页是 js 动态加载的
- 使用了iframe框架

所以，
首先，网页请求不能使用requests库，需要使用Selenium + PhatomJS。
其次，使用Selenium + PhatomJS后，还需要针对 iframe 做特定处理。

废话不多说，看实际操作步骤：
首先打开网页 [http://music.163.com](http://music.163.com/)

![img](https://upload-images.jianshu.io/upload_images/3879861-0cccf8aa6a3862bd.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

在右上角的搜索框中输入“The Beatles”，然后会有一个下拉选项，选择歌手 The Beatles （红框中的内容）。

![img](https://upload-images.jianshu.io/upload_images/3879861-e19f2adccf29f375.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

然后看到如下页面，选择红框中的“所有专辑”，点击。

![img](https://upload-images.jianshu.io/upload_images/3879861-26748f5f0357fd4d.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

这样就会看见所有的专辑列表，以及下方的翻页按钮。

![img](https://upload-images.jianshu.io/upload_images/3879861-0d864dff94d2bff5.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

我们需要的就是所有专辑的图片、专辑名和专辑出版时间。看到这就可以构想一下爬虫的爬取逻辑了。定位到该页面，然后获取页码，然后挨个请求页面来爬取页面中的内容。

点击一下翻页按钮看看url 有没有什么规律。

![img](https://upload-images.jianshu.io/upload_images/3879861-941355c737f65133.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

点击第二页后，看到上面的地址栏！！！看到这个地址栏我都懒得翻页了。。。

limit 参数是限制一个页面加载专辑的个数
offset 参数是前面过滤多少个专辑，现在是一页12个专辑，所以第二页是offset=12，第三页offset=24，以此类推。。。

一共9页，一页12个，也不到120个。So... ... 改一下url 就不用翻页了！！

limit 参数等于120，offset 参数 等于0，就搞定了！ 输入下面的url，看看是不是所有的专辑都加载出来了。

```html
http://music.163.com/#/artist/album?id=101988&limit=120&offset=0
```

下面就开始爬虫代码了。
这里我们会用到上一篇博文中写好的几个工具方法：

```python
    def save_img(self, url, file_name): ##保存图片
        print('开始请求图片地址，过程会有点长...')
        img = self.request(url)
        print('开始保存图片')
        f = open(file_name, 'ab')
        f.write(img.content)
        print(file_name,'图片保存成功！')
        f.close()

    def request(self, url):  #封装的requests 请求
        r = requests.get(url)  # 像目标url地址发送get请求，返回一个response对象。有没有headers参数都可以。
        return r

    def mkdir(self, path):  ##这个函数创建文件夹
        path = path.strip()
        isExists = os.path.exists(path)
        if not isExists:
            print('创建名字叫做', path, '的文件夹')
            os.makedirs(path)
            print('创建成功！')
            return True
        else:
            print(path, '文件夹已经存在了，不再创建')
            return False

    def get_files(self, path): #获取文件夹中的文件名称列表
        pic_names = os.listdir(path)
        return pic_names
```

OK, 开始我们的爬虫逻辑部分：

这里值得注意的是，该页面使用frame 框架，使用Selenium + PhantomJS 后并不会加载iframe 框架中的网页内容。iframe 框架相当于在页面中又加载了一个页面，需要使用Selenium 的 switch_to.frame() 方法加载（官网给的方法是switch_to_frame()，但是IDE提醒使用前面的方法替代该方法）。

看下面的网页结构，iframe的id是“g_iframe”：
![img](https://upload-images.jianshu.io/upload_images/3879861-0dd552f45f26ea5b.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1240)


加载 iframe 框架中的内容： ``` driver = webdriver.PhantomJS() driver.get(self.init_url) driver.switch_to.frame("g_iframe") html = driver.page_source ```

然后找到所有的封面元素：

![img](https://upload-images.jianshu.io/upload_images/3879861-7f0923c0aec8375c.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

根据上图的网页结构可以看出，所有的专辑信息都在ul 标签里面，每一个专辑在一个li 标签里。li 标签中包含了图片url、专辑名字、以及专辑时间。

抓取其中的内容就好了。

```python
all_li = BeautifulSoup(html, 'lxml').find(id='m-song-module').find_all('li')

for li in all_li:
    album_img = li.find('img')['src']
    album_name = li.find('p', class_='dec')['title']
    album_date = li.find('span', class_='s-fc3').get_text()
```

这里获取到的图片url 依然是有图片宽高参数的，所以要过滤宽高参数：
`http://p4.music.126.net/pLA1GX0KtU-vU4ZA6Cr-OQ==/1401877340532770.jpg?param=120y120`

把问号后面的参数过滤掉：

```python
end_pos = album_img.index('?')  #找到问号的位置
album_img_url = album_img[:end_pos]  #截取问号之前的内容
```

图片命名逻辑：专辑时间 + 专辑名。
专辑名可能有一些特殊字符，需要替换掉！
`photo_name = album_date + ' - ' + album_name.replace('/','').replace(':',',') + '.jpg'`

再使用上一篇博文例子中的去重逻辑，修改后的爬虫逻辑部分如下：

```python
    def spider(self):
        print("Start!")
        driver = webdriver.PhantomJS()
        driver.get(self.init_url)
        driver.switch_to.frame("g_iframe")
        html = driver.page_source

        self.mkdir(self.folder_path)  # 创建文件夹
        print('开始切换文件夹')
        os.chdir(self.folder_path)  # 切换路径至上面创建的文件夹

        file_names = self.get_files(self.folder_path)  # 获取文件夹中的所有文件名，类型是list

        all_li = BeautifulSoup(html, 'lxml').find(id='m-song-module').find_all('li')
        # print(type(all_li))

        for li in all_li:
            album_img = li.find('img')['src']
            album_name = li.find('p', class_='dec')['title']
            album_date = li.find('span', class_='s-fc3').get_text()
            end_pos = album_img.index('?')
            album_img_url = album_img[:end_pos]

            photo_name = album_date + ' - ' + album_name.replace('/','').replace(':',',') + '.jpg'
            print(album_img_url, photo_name)

            if photo_name in file_names:
                print('图片已经存在，不再重新下载')
            else:
                self.save_img(album_img_url, photo_name)
```

其实相对于上篇博文的例子，这个爬虫的逻辑部分还是挺简洁的。

**最后上一个完整的代码：** [也可以从GitHub下载](https://github.com/AlbertShoubinLi/TheBeatles/tree/f02d6be5db585d3924c736b3976e8302d71afbd9)

```python
from selenium import webdriver
from bs4 import BeautifulSoup
import requests
import os

class AlbumCover():

    def __init__(self):
        self.init_url = "http://music.163.com/#/artist/album?id=101988&limit=120&offset=0" #请求网址
        self.folder_path = "C:\D\TheBeatles" #想要存放的文件目录

    def save_img(self, url, file_name):  ##保存图片
        print('开始请求图片地址，过程会有点长...')
        img = self.request(url)
        print('开始保存图片')
        f = open(file_name, 'ab')
        f.write(img.content)
        print(file_name, '图片保存成功！')
        f.close()

    def request(self, url):  # 封装的requests 请求
        r = requests.get(url)  # 像目标url地址发送get请求，返回一个response对象。有没有headers参数都可以。
        return r

    def mkdir(self, path):  ##这个函数创建文件夹
        path = path.strip()
        isExists = os.path.exists(path)
        if not isExists:
            print('创建名字叫做', path, '的文件夹')
            os.makedirs(path)
            print('创建成功！')
            return True
        else:
            print(path, '文件夹已经存在了，不再创建')
            return False

    def get_files(self, path):  # 获取文件夹中的文件名称列表
        pic_names = os.listdir(path)
        return pic_names

    def spider(self):
        print("Start!")
        driver = webdriver.PhantomJS()
        driver.get(self.init_url)
        driver.switch_to.frame("g_iframe")
        html = driver.page_source

        self.mkdir(self.folder_path)  # 创建文件夹
        print('开始切换文件夹')
        os.chdir(self.folder_path)  # 切换路径至上面创建的文件夹

        file_names = self.get_files(self.folder_path)  # 获取文件夹中的所有文件名，类型是list

        all_li = BeautifulSoup(html, 'lxml').find(id='m-song-module').find_all('li')
        # print(type(all_li))

        for li in all_li:
            album_img = li.find('img')['src']
            album_name = li.find('p', class_='dec')['title']
            album_date = li.find('span', class_='s-fc3').get_text()
            end_pos = album_img.index('?')
            album_img_url = album_img[:end_pos]

            photo_name = album_date + ' - ' + album_name.replace('/', '').replace(':', ',') + '.jpg'
            print(album_img_url, photo_name)

            if photo_name in file_names:
                print('图片已经存在，不再重新下载')
            else:
                self.save_img(album_img_url, photo_name)

album_cover = AlbumCover()
album_cover.spider()
```

执行结果：

![img](https://upload-images.jianshu.io/upload_images/3879861-7a10ba76b4380ac9.png?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

看看文件夹里面什么样：

![img](https://upload-images.jianshu.io/upload_images/3879861-7963f71b09d84f15.png?imageMogr2/auto-orient/strip|imageView2/2/w/1240)

历年的专辑封面已经到手啦，还有专辑的名称和发行日期。

